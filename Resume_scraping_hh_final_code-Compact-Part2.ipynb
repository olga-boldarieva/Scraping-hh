{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "This part of the program takes unique resume identification numbers collected in Part 1 and sends API requests to get all the necessary information about every job-seeker and job postings similar to his/her resume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize data writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init_data_writer(data_path, col_names): \n",
    "    ''' Initialize data file.\n",
    "    \n",
    "    Arguments:\n",
    "        data_path (str) -- Relative or absolute path to data file.\n",
    "        col_names (list) -- Column names to be exported with data.\n",
    "    \n",
    "    Returns: io Buffered Writer to tab-separated data file with column names.\n",
    "    '''\n",
    "    with open(data_path, 'wb') as dataWriter:\n",
    "        header = \"\\t\".join(col_names) + '\\n'\n",
    "        dataWriter.write(header.encode('utf-8'))\n",
    "    return dataWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write data to data writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_writer(data_path, data_row):\n",
    "    '''Write data to file.\n",
    "    \n",
    "    Arguments:\n",
    "        data_path (str) -- Relative or absolute path to data file.\n",
    "        data_row (list) -- Data to be added to data file.\n",
    "\n",
    "    Returns: io Buffered Writer to data file.\n",
    "    '''\n",
    "    with open(data_path, \"ab\") as toWrite:\n",
    "        for item in data_row:\n",
    "            row = \"\\t\".join(item) + '\\n'\n",
    "            toWrite.write(row.encode('utf-8'))\n",
    "    return toWrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine several csv files into one big csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import csv\n",
    "\n",
    "def combine_files(data_file_path, csv_path, date_prefix_dir):\n",
    "    ''' Combine all csv files from the same folder into one csv file.\n",
    "    \n",
    "    Arguments:\n",
    "        data_file_path (str) -- Relative or absolute path to data file.\n",
    "        csv_path (str) -- Relative or absolute path to csv files that need to be combined.\n",
    "        date_prefix_dir (str) -- Relative or absolute path to a directory for a given day.  \n",
    "        \n",
    "    Returns: io Buffered Writer to csv file. \n",
    "    '''\n",
    "    with open(data_file_path, 'a') as singleFile:\n",
    "        for csv in glob(csv_path):\n",
    "            if csv == \"main_\" + date_prefix_dir + \".csv\":\n",
    "                pass\n",
    "            else:\n",
    "                for line in open(csv, 'r'):\n",
    "                    singleFile.write(line)\n",
    "    return data_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding relevant vacancies for every applicant from API search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def Vacancies_search(url, area, specialization_1, specialization_2, specialization_3):\n",
    "    ''' Save relevent information about vacancies (job postings).\n",
    "\n",
    "    Arguments:\n",
    "        url (str) -- URL to search job posting with specific parameters.\n",
    "        area (str) -- code of the city where to search for job postings.\n",
    "        specialization_1 (str) -- specilization 1 mentioned in a job posting.\n",
    "        specialization_2 (str) -- specialization 2 mentioned in a job posting.\n",
    "        specialization_3 (str) -- specialization 3 mentioned in a job posting.\n",
    "\n",
    "    Returns: list object that contains the number of job postings found,  \n",
    "    mean, minimum, and maximum wage levels mentioned in the job postings found.\n",
    "    '''\n",
    "    if specialization_2 != '0':\n",
    "        if specialization_3 != '0':\n",
    "            url_search = url + '&area=' + str(area) + '&specialization=' + str(specialization_1) + \\\n",
    "            '&specialization=' + str(specialization_2) + '&specialization=' + str(specialization_3) + \\\n",
    "            '&employment=full&only_with_salary=true&order_by=relevance&per_page=500&page=0'\n",
    "        else:\n",
    "            url_search = url + '&area=' + str(area) + '&specialization=' + str(specialization_1) + \\\n",
    "            '&specialization=' + str(specialization_2) + \\\n",
    "            '&employment=full&only_with_salary=true&order_by=relevance&per_page=500&page=0'\n",
    "    else:\n",
    "        url_search = url + '&area=' + str(area) + '&specialization=' + str(specialization_1) + \\\n",
    "        '&employment=full&only_with_salary=true&order_by=relevance&per_page=500&page=0'\n",
    "\n",
    "    headers = {'User-Agent': 'career-success (olga.boldareva@gmail.com)'}\n",
    "    response = requests.get(url_search, headers=headers)\n",
    "    search_results = json.loads(response.text)\n",
    "    salary_av_list = []\n",
    "    for i in range(len(search_results['items'])):\n",
    "        if (search_results['items'][i]['salary']['currency']) == 'RUR':\n",
    "            salary_from = search_results['items'][i]['salary']['from']\n",
    "            salary_to = search_results['items'][i]['salary']['to']\n",
    "            if salary_to == None:\n",
    "                salary_av = salary_from\n",
    "            elif salary_from == None:\n",
    "                salary_av = salary_to\n",
    "            else: salary_av = (salary_to + salary_from)/2.0\n",
    "            salary_av_list.append(salary_av)\n",
    "    salary_list = [str(len(salary_av_list))]\n",
    "    if len(salary_av_list) > 0:\n",
    "        salary_mean = round(sum(salary_av_list) / float(len(salary_av_list)), 2)\n",
    "        salary_list.append(str(salary_mean))\n",
    "        salary_min = min(salary_av_list)\n",
    "        salary_list.append(str(salary_min))\n",
    "        salary_max = max(salary_av_list)\n",
    "        salary_list.append(str(salary_max))\n",
    "    return salary_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting additional information about applicants from API requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def API_resume_information(url_resume):\n",
    "    '''Collect all the necessary information about an applicant using his/her resume \n",
    "    and job postings similar to this resume.\n",
    "\n",
    "    Arguments:\n",
    "        url_resume (str) -- URL to access all the information available about a job-seeker.\n",
    "\n",
    "    Returns: list object that containes information about a job-seeker and relevant job postings.\n",
    "    '''\n",
    "    headers = {'User-Agent': 'career-success (olga.boldareva@gmail.com)'}\n",
    "    response = requests.get(url_resume, headers=headers)\n",
    "    resume = json.loads(response.text)\n",
    "    resume_inf = [str(response.status_code)]\n",
    "    if response.status_code == 200:\n",
    "        if resume['birth_date'] is not None:\n",
    "            resume_inf.append(resume['birth_date'])\n",
    "        else:\n",
    "            resume_inf.append('None')\n",
    "        if resume['gender'] is not None:\n",
    "            resume_inf.append(resume['gender']['id'])\n",
    "        else:\n",
    "            resume_inf.append('None')\n",
    "        area = resume['area']['id']\n",
    "        if resume['area'] is not None:\n",
    "            resume_inf.append(area)\n",
    "        else:\n",
    "            resume_inf.append('None')\n",
    "        text = resume['title']\n",
    "        if resume['title'] is not None:\n",
    "            resume_inf.append(text)\n",
    "        else:\n",
    "            resume_inf.append('None')\n",
    "        if resume['specialization'] is not None:\n",
    "            specialization_number = len(resume['specialization'])\n",
    "        else:\n",
    "            specialization_number = 0\n",
    "        specializations = []\n",
    "        if specialization_number > 0:\n",
    "            for i, item in enumerate(resume['specialization']):\n",
    "                if i <= 2:\n",
    "                    resume_inf.append(item['id'])\n",
    "                    specializations.append(item['id'])\n",
    "                else:\n",
    "                    pass\n",
    "        if len(resume['specialization']) < 3:\n",
    "            for j in range(specialization_number + 1, 4):\n",
    "                resume_inf.append('0')\n",
    "                specializations.append('0')\n",
    "        specialization = resume['specialization'][0]['id']\n",
    "        specialization_1 = specializations[0]\n",
    "        specialization_2 = specializations[1]\n",
    "        specialization_3 = specializations[2]\n",
    "\n",
    "        if resume['salary'] is not None:\n",
    "            resume_inf.append(str(resume['salary']['amount']))\n",
    "        else:\n",
    "            resume_inf.append('None')\n",
    "        if resume['salary'] is not None:\n",
    "            resume_inf.append(resume['salary']['currency'])\n",
    "        else:\n",
    "            resume_inf.append('None')\n",
    "        if resume['employment'] is not None:\n",
    "            resume_inf.append(resume['employment']['id'])\n",
    "        else:\n",
    "            resume_inf.append('None')\n",
    "        if resume['education'] is not None:\n",
    "            resume_inf.append(resume['education']['level']['id'])\n",
    "        else:\n",
    "            resume_inf.append('None')\n",
    "        if resume['total_experience'] is not None:\n",
    "            resume_inf.append(str(resume['total_experience']['months']))\n",
    "        else:\n",
    "            resume_inf.append('0')\n",
    "        if resume['citizenship'] is not None:\n",
    "            resume_inf.append(resume['citizenship'][0]['id'])\n",
    "        else:\n",
    "            resume_inf.append('None')\n",
    "        if specialization_1 != '0':\n",
    "            url = 'https://api.hh.ru/vacancies?'\n",
    "            try:\n",
    "                salary_row = Vacancies_search(url, area, specialization_1, specialization_2, specialization_3)\n",
    "            except:\n",
    "                salary_row = []\n",
    "            resume_inf = resume_inf + salary_row\n",
    "    return resume_inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending API requests and saving information in txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from time import sleep\n",
    "import datetime as dt\n",
    "\n",
    "def API_requests(date_prefix_dir, data, num_columns, num_rows):\n",
    "    '''Send reguests to hh.ru API using unique resume identification numbers \n",
    "    provided in a DataFrame.\n",
    "\n",
    "    Arguments:\n",
    "        date_prefix_dir (str) -- date identifier.\n",
    "        data (DataFrame) -- DataFrame that contains unique id for every applicant \n",
    "        and all resume identification numbers he/she posted on the website.  \n",
    "        num_columns (int) -- number of columns in a DataFrame provided.\n",
    "        num_rows (int) -- number of rows in a DataFrame provided.\n",
    "\n",
    "    Returns: io Buffered Writer to txt file with complete information about an applicant \n",
    "    and related job postings.\n",
    "    '''\n",
    "    # now=dt.datetime.today()\n",
    "    # print(\"This is when I start. Now is {}\".format(now))\n",
    "\n",
    "    url = 'https://api.hh.ru/resumes/'\n",
    "    headers = {'User-Agent': 'career-success (olga.boldareva@gmail.com)'}\n",
    "\n",
    "    resumes_path = \"hh_data/hh_resumes/\"\n",
    "    data_path = resumes_path + \"final_datasets/final_\" + date_prefix_dir + \".txt\"\n",
    "    col_names = [\"id\", \"resumeId\", \"response_code\", \"birth_date\", \"gender\", \"area\", \"title\", \\\n",
    "             \"specialization_1\", \"specialization_2\", \"specialization_3\", \"salary_amount\", \\\n",
    "             \"salary_currency\", \"employment\", \"education_level\", \"total_experience\", \\\n",
    "             \"citizenship\", \"number_vac\", \"salary_av_vac\", \"salary_min_vac\", \"salary_max_vac\"]\n",
    "    file_initialized = init_data_writer(data_path, col_names)\n",
    "\n",
    "    #for i in range(1, num_rows + 1):\n",
    "    for i in range(1, 11):\n",
    "        j = 1\n",
    "        column = \"resumeId_\" + str(j)\n",
    "        while (j <= num_columns)&(data.loc[[i],[column]].notnull().any().any() == True):\n",
    "            data_res = [str(data.loc[[i],['id']].values[0][0])]\n",
    "            data_res.append(data.loc[[i],[column]].values[0][0])\n",
    "            url_resume = url + data_res[1]\n",
    "            try:\n",
    "                data_res = data_res + API_resume_information(url_resume)\n",
    "            except:\n",
    "                pass\n",
    "            data_row = [data_res]\n",
    "            data_file = data_writer(data_path, data_row)\n",
    "\n",
    "            j = j + 1\n",
    "            if j <= num_columns:\n",
    "                column = \"resumeId_\" + str(j)\n",
    "            sleep(1)\n",
    "\n",
    "    # now=dt.datetime.today()\n",
    "    # print(\"{} resumes were scraped\".format(num_rows))\n",
    "    # print(\"This is when I end. Now is {}\".format(now))\n",
    "\n",
    "    return data_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all files together, delete duplicates, and convert csv file to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def data_file():\n",
    "    '''Combine csv files, delete duplicates, convert csv to dataframe, and save main csv files.\n",
    "    \n",
    "    No arguments.\n",
    "        \n",
    "    Returns: io Buffered Writer to data file.\n",
    "    '''\n",
    "\n",
    "    #print(\"data_file function is about to start!\")\n",
    "\n",
    "    resumes_path = \"hh_data/hh_resumes/\"\n",
    "    now = dt.datetime.now()\n",
    "    date_prefix_dir = now.strftime('%Y-%m-%d')\n",
    "    date_prefix_id = now.strftime('%m%d')\n",
    "    resumes_directory = resumes_path + date_prefix_dir\n",
    "    data_file_path = resumes_directory + \"/main_\" + date_prefix_dir + \".csv\"\n",
    "    data_file_path_no_dup = resumes_directory + \"/main_\" + date_prefix_dir + \"_no_duplicates.csv\"\n",
    "\n",
    "    if os.path.exists(data_file_path):\n",
    "        os.remove(data_file_path)\n",
    "    if os.path.exists(data_file_path_no_dup):\n",
    "        os.remove(data_file_path_no_dup)\n",
    "\n",
    "    csv_path = resumes_directory + \"/*.csv\"\n",
    "    combine_files(data_file_path, csv_path, date_prefix_dir)\n",
    "\n",
    "    with open(data_file_path,\"r\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        columns = []\n",
    "        for row in reader:\n",
    "            columns.append(len(row))\n",
    "        num_columns = max(columns)\n",
    "\n",
    "    names_columns = []\n",
    "    for i in range(num_columns):\n",
    "        name = 'resumeId_' + str(i+1)\n",
    "        names_columns.append(name)\n",
    "\n",
    "    df = pd.read_csv(data_file_path, sep = ',',\n",
    "    header = None,\n",
    "    usecols = range(num_columns),\n",
    "    names = names_columns,\n",
    "    encoding = 'utf-8',\n",
    "    dtype = str,\n",
    "    low_memory = False)\n",
    "\n",
    "    data = df.drop_duplicates(keep = 'first') #drop all row duplicates\n",
    "\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    idx = pd.Int64Index(range(1, len(data) + 1))\n",
    "    data.index = idx\n",
    "    data['id'] = int(date_prefix_id)*100000 + data.index\n",
    "    num_rows = data.shape[0]\n",
    "    data.to_csv(data_file_path_no_dup, sep=',', index = False, encoding='utf-8')\n",
    "    #print(data[0:6])\n",
    "\n",
    "    data_file = API_requests(date_prefix_dir, data, num_columns, num_rows)\n",
    "    return data_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set timer\n",
    "While resume numbers are being collected during the whole day, working with data should start at the end of the day. This program starts at 11.55 pm and works with all the data gathered during the day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delay is 75.236577\n",
      "data_file function is about to start!\n",
      "                               resumeId_1 resumeId_2 resumeId_3 resumeId_4  \\\n",
      "1  1ecfcee700034fa06b0039ed1f5854754b5152        NaN        NaN        NaN   \n",
      "2  6b3f75d00003b360c10039ed1f32756c4b4437        NaN        NaN        NaN   \n",
      "3  1d42fee200023043a40039ed1f55334f315468        NaN        NaN        NaN   \n",
      "4  b4b0d1b20003685d3b0039ed1f34534d4c6761        NaN        NaN        NaN   \n",
      "5  a6f27d8800021c15510039ed1f757157485875        NaN        NaN        NaN   \n",
      "6  2baeeab000004c4ae80039ed1f736563726574        NaN        NaN        NaN   \n",
      "\n",
      "  resumeId_5 resumeId_6 resumeId_7 resumeId_8 resumeId_9 resumeId_10  \\\n",
      "1        NaN        NaN        NaN        NaN        NaN         NaN   \n",
      "2        NaN        NaN        NaN        NaN        NaN         NaN   \n",
      "3        NaN        NaN        NaN        NaN        NaN         NaN   \n",
      "4        NaN        NaN        NaN        NaN        NaN         NaN   \n",
      "5        NaN        NaN        NaN        NaN        NaN         NaN   \n",
      "6        NaN        NaN        NaN        NaN        NaN         NaN   \n",
      "\n",
      "  resumeId_11 resumeId_12 resumeId_13 resumeId_14 resumeId_15        id  \n",
      "1         NaN         NaN         NaN         NaN         NaN  40500001  \n",
      "2         NaN         NaN         NaN         NaN         NaN  40500002  \n",
      "3         NaN         NaN         NaN         NaN         NaN  40500003  \n",
      "4         NaN         NaN         NaN         NaN         NaN  40500004  \n",
      "5         NaN         NaN         NaN         NaN         NaN  40500005  \n",
      "6         NaN         NaN         NaN         NaN         NaN  40500006  \n",
      "This is when I start. Now is 2017-04-05 16:31:01.054849\n",
      "197 resumes were scraped\n",
      "This is when I end. Now is 2017-04-05 16:32:14.616673\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from threading import Timer\n",
    "\n",
    "def timer_fnc(delay):\n",
    "    '''Set timer to run data_file() function at 11.55 pm.\n",
    "    \n",
    "    Argument:\n",
    "        delay (float) -- number of seconds to wait until a given function should be called.  \n",
    "        \n",
    "    Returns: Timer Thread.\n",
    "    '''\n",
    "    t = Timer(delay, data_file)\n",
    "    t.start()\n",
    "    return t\n",
    "\n",
    "\n",
    "now=dt.datetime.today()\n",
    "\n",
    "nextTime = now.replace(day = now.day, hour=16, minute=31, second=0, microsecond=0)\n",
    "#nextTime = now.replace(day = now.day, hour=23, minute=55, second=0, microsecond=0)\n",
    "if now < nextTime:\n",
    "    delay = (nextTime - now).total_seconds() + 1\n",
    "    print(\"Delay is {}\".format(delay))\n",
    "    timer_fnc(delay)  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
